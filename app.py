from dotenv import load_dotenv
import os
import streamlit as st
import cohere

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings.base import Embeddings  # for custom embeddings wrapper

# Load environment variables from .env file
load_dotenv()

# Fetch Cohere API key from environment
cohere_api_key = os.getenv("COHERE_API_KEY")
if not cohere_api_key:
    st.error("COHERE_API_KEY environment variable not found. Please set it before running the app.")
    st.stop()

# Initialize Cohere client
co = cohere.Client(cohere_api_key)

st.set_page_config(page_title="PDF Q&A with Cohere", layout="wide")
print("Loaded COHERE_API_KEY:", cohere_api_key)  # Debug print, remove in production


@st.cache_data(show_spinner=False)
def load_pdf_chunks(pdf_path):
    """Load PDF and split into chunks."""
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(pages)
    return chunks


class CohereEmbeddings(Embeddings):
    """Custom Cohere embeddings wrapper compatible with LangChain."""

    def embed_documents(self, texts):
        try:
            response = co.embed(texts=texts, model="embed-english-v2.0")
            return response.embeddings
        except Exception as e:
            st.error(f"Error during embedding: {e}")
            return [[] for _ in texts]

    def embed_query(self, text):
        res = co.embed(texts=[text], model="embed-english-v2.0")
        return res.embeddings[0]


def get_vectorstore(chunks):
    embeddings = CohereEmbeddings()
    vectorstore = Chroma.from_documents(chunks, embeddings)
    return vectorstore


def answer_query(question, relevant_docs):
    """
    Use retrieved chunks as context and get answer from Cohere chat model.
    """
    context = "\n\n".join([doc.page_content for doc in relevant_docs])

    prompt = f"""Answer the following question using only the provided context.

Context:
{context}

Question: {question}
Answer:"""

    try:
        response = co.chat(
            model="command-xlarge-nightly",
            message=prompt,
            max_tokens=500,
            temperature=0
        )
        return response.text.strip()
    except Exception as e:
        return f"Error generating answer: {e}"


def main():
    st.title("ðŸ“„ PDF Question & Answer with Cohere")

    st.markdown(
        """
        Upload a PDF file, ask questions about its content, and get answers generated by Cohere
        based solely on the content in the PDF using semantic search.
        """
    )

    uploaded_file = st.file_uploader("Upload your PDF file", type=["pdf"])

    if uploaded_file is not None:
        # Save uploaded PDF temporarily
        with open("temp_uploaded.pdf", "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.success("PDF uploaded successfully!")

        # Load and chunk PDF
        chunks = load_pdf_chunks("temp_uploaded.pdf")

        # Create vectorstore from chunks (embedding + indexing)
        vectorstore = get_vectorstore(chunks)

        # Input question from user
        question = st.text_input("Ask a question based on the PDF content:")

        if st.button("Get Answer") and question.strip() != "":
            with st.spinner("Searching and generating answer..."):
                # Find relevant chunks by semantic similarity
                relevant_docs = vectorstore.similarity_search(question, k=3)

                # Get answer using Cohere chat API
                answer = answer_query(question, relevant_docs)

            st.markdown("### Answer:")
            st.write(answer)


if __name__ == "__main__":
    main()
